{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0RH3TX5KMrM","executionInfo":{"status":"ok","timestamp":1739857604768,"user_tz":-540,"elapsed":22206,"user":{"displayName":"ì‹ ì§„ì„­","userId":"17065963932824802147"}},"outputId":"40327947-8b74-4fba-848e-ba765e6ffea2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/bit_conference/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ftayCrzQKN4u","executionInfo":{"status":"ok","timestamp":1739857604769,"user_tz":-540,"elapsed":5,"user":{"displayName":"ì‹ ì§„ì„­","userId":"17065963932824802147"}},"outputId":"8b0e02c7-a468-4307-94e4-048277fad795"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1YDrmXvwQeDTF3AVegVo_-qlULY2-1-qE/bit_conference\n"]}]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"X3gqSsKWLhkl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ì´ë¯¸ì§€ í´ë” ì •ë¦¬"],"metadata":{"id":"6dfnIDhTiL2u"}},{"cell_type":"code","source":["import os\n","import shutil\n","\n","data_dir = \"/content/gdrive/MyDrive/bit_conference/image_gen\"\n","emotions = [\"Exciting\", \"Hopeful\", \"Romantic\", \"Heartwarming\", \"Calm\", \"Sad\", \"Stress\", \"Lonely\"]\n","\n","for emotion in emotions:\n","    lower_emotion = emotion.lower()\n","    src_folder = os.path.join(data_dir, lower_emotion)  # ì˜ˆ: 'exciting'\n","    dst_folder = os.path.join(data_dir, lower_emotion + \"_aug\")  # ì˜ˆ: 'exciting_aug'\n","    renamed_folder = os.path.join(data_dir, emotion)  # ì˜ˆ: 'Exciting'\n","\n","    # ì´ë¯¸ì§€ íŒŒì¼ ì´ë™\n","    if os.path.exists(src_folder) and os.path.exists(dst_folder):\n","        for file_name in os.listdir(src_folder):\n","            src_file = os.path.join(src_folder, file_name)\n","            dst_file = os.path.join(dst_folder, file_name)\n","            if os.path.isfile(src_file):  # íŒŒì¼ë§Œ ì´ë™\n","                shutil.move(src_file, dst_file)\n","\n","        # ì›ë³¸ í´ë” ì‚­ì œ (ì„ íƒì‚¬í•­)\n","        os.rmdir(src_folder)\n","\n","        # dst_folder ì´ë¦„ ë³€ê²½\n","        os.rename(dst_folder, renamed_folder)\n","    else:\n","        print(f\"Skipping {emotion}: One or both folders not found.\")\n","\n","print(\"Processing complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8mxe-UmGU4AV","executionInfo":{"status":"ok","timestamp":1739819571289,"user_tz":-540,"elapsed":10720,"user":{"displayName":"ì‹ ì§„ì„­","userId":"17065963932824802147"}},"outputId":"b4c305b4-ebdc-450e-c482-f43a74b2ba09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing complete.\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","\n","# ì„¤ì •ëœ ê²½ë¡œ\n","base_path = \"/content/gdrive/MyDrive/bit_conference/\"\n","source_folder = os.path.join(base_path, \"image_gen\")\n","target_folder = os.path.join(base_path, \"image_aug\")\n","\n","# ê°ì • í´ë” ë¦¬ìŠ¤íŠ¸\n","EMOTIONS = [\"Exciting\", \"Hopeful\", \"Romantic\", \"Heartwarming\", \"Calm\", \"Sad\", \"Stress\", \"Lonely\"]\n","\n","# ìƒˆë¡œìš´ image_aug í´ë” ë° í•˜ìœ„ í´ë” ìƒì„±\n","os.makedirs(target_folder, exist_ok=True)\n","for emotion in EMOTIONS:\n","    os.makedirs(os.path.join(target_folder, emotion), exist_ok=True)\n","\n","# ê° ê°ì • í´ë”ë¥¼ ìˆœíšŒí•˜ë©° '_aug'ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ íŒŒì¼ ì´ë™\n","for emotion in EMOTIONS:\n","    source_emotion_folder = os.path.join(source_folder, emotion)\n","    target_emotion_folder = os.path.join(target_folder, emotion)\n","\n","    if os.path.exists(source_emotion_folder):\n","        for filename in os.listdir(source_emotion_folder):\n","            if \"_aug\" in filename and filename.endswith(\".png\"):\n","                src_path = os.path.join(source_emotion_folder, filename)\n","                dst_path = os.path.join(target_emotion_folder, filename)\n","                shutil.move(src_path, dst_path)\n","                print(f\"Moved: {src_path} -> {dst_path}\")\n","\n","print(\"All '_aug' images have been moved successfully.\")"],"metadata":{"id":"r1mJmX44nWfN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# image_augì˜ í•˜ìœ„ í´ë”ì—ì„œ íŒŒì¼ëª…ì˜ ë§ˆì§€ë§‰ ê¸€ìê°€ '2'ì¸ png íŒŒì¼ ì‚­ì œ\n","for emotion in EMOTIONS:\n","    target_emotion_folder = os.path.join(target_folder, emotion)\n","    if os.path.exists(target_emotion_folder):\n","        for filename in os.listdir(target_emotion_folder):\n","            if filename.endswith(\"2.png\"):\n","                file_path = os.path.join(target_emotion_folder, filename)\n","                os.remove(file_path)\n","                print(f\"Deleted: {file_path}\")\n","\n","print(\"All images ending with '2.png' have been deleted successfully.\")"],"metadata":{"id":"XfNu_vp6v-1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Efficientnet íŒŒì¸íŠœë‹"],"metadata":{"id":"1Dbk5OBmiNdn"}},{"cell_type":"code","source":["import os\n","import torch\n","import random\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n","\n","# ë°ì´í„°ì…‹ ê²½ë¡œ ë° ê°ì • ë ˆì´ë¸” ì •ì˜\n","DATASET_PATH = \"/content/gdrive/MyDrive/bit_conference/image_aug\"\n","EMOTIONS = [\"Exciting\", \"Hopeful\", \"Romantic\", \"Heartwarming\", \"Calm\", \"Sad\", \"Stress\", \"Lonely\"]\n","\n","# ì´ë¯¸ì§€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n","def load_dataset():\n","    dataset = []\n","    for label, emotion in enumerate(EMOTIONS):\n","        emotion_path = os.path.join(DATASET_PATH, emotion)\n","        for img_name in os.listdir(emotion_path):\n","            img_path = os.path.join(emotion_path, img_name)\n","            dataset.append((img_path, label))\n","    random.shuffle(dataset)  # ë°ì´í„° ìˆœì„œ ì„ê¸°\n","    return dataset\n","\n","class EmotionDataset(Dataset):\n","    def __init__(self, dataset, processor):\n","        self.dataset = dataset\n","        self.processor = processor\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        img_path, label = self.dataset[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","        inputs = self.processor(image, return_tensors=\"pt\")\n","        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n","        inputs[\"labels\"] = torch.tensor(int(label), dtype=torch.int64)\n","        return inputs\n","\n","# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n","dataset = load_dataset()\n","random.shuffle(dataset)\n","train_size = int(0.8 * len(dataset))\n","train_data, val_data = dataset[:train_size], dataset[train_size:]\n","\n","# ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ë¡œë“œ\n","processor = AutoImageProcessor.from_pretrained(\"google/efficientnet-b0\")\n","train_dataset = EmotionDataset(train_data, processor)\n","val_dataset = EmotionDataset(val_data, processor)\n","\n","# ëª¨ë¸ ë¡œë“œ ë° ìˆ˜ì •\n","model = AutoModelForImageClassification.from_pretrained(\n","    \"google/efficientnet-b0\",\n","    num_labels=len(EMOTIONS),\n","    ignore_mismatched_sizes=True\n",")\n","\n","# ResNet Backbone Freeze\n","for param in model.efficientnet.parameters():\n","    param.requires_grad = False\n","\n","# Fully Connected Layerë§Œ í•™ìŠµ\n","for param in model.classifier.parameters():\n","    param.requires_grad = True\n","\n","# í›ˆë ¨ ì„¤ì •\n","training_args = TrainingArguments(\n","    output_dir=\"/content/gdrive/MyDrive/bit_conference/efficientnet_emotion_model\",\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=2,\n","    per_device_eval_batch_size=16,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    num_train_epochs=5,\n","    logging_dir=\"/content/gdrive/MyDrive/bit_conference/logs\",\n","    logging_steps=10,\n","    save_total_limit=2,\n","    fp16=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n",")\n","\n","# ëª¨ë¸ í•™ìŠµ\n","trainer.train()\n","\n","# ëª¨ë¸ ì €ì¥\n","model.save_pretrained(\"/content/gdrive/MyDrive/bit_conference/efficientnet_emotion_model\")\n","processor.save_pretrained(\"/content/gdrive/MyDrive/bit_conference/efficientnet_emotion_model\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"id":"FGED-ZySk66-","executionInfo":{"status":"ok","timestamp":1739829766151,"user_tz":-540,"elapsed":2799244,"user":{"displayName":"ì‹ ì§„ì„­","userId":"17065963932824802147"}},"outputId":"df061780-cce4-4cc2-804c-713872df3c67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of EfficientNetForImageClassification were not initialized from the model checkpoint at google/efficientnet-b0 and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([1000, 1280]) in the checkpoint and torch.Size([8, 1280]) in the model instantiated\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([8]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1750/1750 46:37, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.987400</td>\n","      <td>2.024227</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.014400</td>\n","      <td>2.003453</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.918700</td>\n","      <td>1.946142</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.857900</td>\n","      <td>1.930461</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["['/content/gdrive/MyDrive/bit_conference/efficientnet_emotion_model/preprocessor_config.json']"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["### ê²°ê³¼"],"metadata":{"id":"xJj0HzqBmNZa"}},{"cell_type":"code","source":["import os\n","import torch\n","import pandas as pd\n","from tqdm import tqdm\n","from PIL import Image\n","from transformers import AutoImageProcessor, AutoModelForImageClassification\n","\n","# ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (ë°˜ì •ë°€ë„ ì—°ì‚° ì ìš©)\n","model_path = \"/content/gdrive/MyDrive/bit_conference/efficientnet_emotion_model\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ëª¨ë¸ì„ ë°˜ì •ë°€ë„ë¡œ ë¡œë“œ\n","model = AutoModelForImageClassification.from_pretrained(model_path).to(device).half()\n","processor = AutoImageProcessor.from_pretrained(model_path)\n","model.eval()\n","\n","# ê°ì • ë ˆì´ë¸” ì •ì˜\n","EMOTIONS = [\"Exciting\", \"Hopeful\", \"Romantic\", \"Heartwarming\", \"Calm\", \"Sad\", \"Stress\", \"Lonely\"]\n","\n","# ì´ë¯¸ì§€ í´ë” ê²½ë¡œ\n","image_gen_path = \"/content/gdrive/MyDrive/bit_conference/image_gen\"\n","\n","# ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬\n","results = {emotion: [] for emotion in EMOTIONS}\n","\n","# ì´ë¯¸ì§€ ì˜ˆì¸¡ í•¨ìˆ˜\n","def predict(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = processor(image, return_tensors=\"pt\")\n","\n","    # GPUë¡œ ì´ë™ ë° ë°˜ì •ë°€ë„ ì ìš© (ë°˜ë“œì‹œ tensorì—ë§Œ ì ìš©)\n","    inputs = {k: v.to(device).half() for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        logits = model(**inputs).logits\n","\n","    probs = torch.nn.functional.softmax(logits, dim=-1).squeeze()\n","    predicted_label = probs.argmax().item()\n","\n","    return predicted_label, probs.cpu().numpy()\n","\n","# ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥ (tqdm ì¶”ê°€)\n","for emotion in EMOTIONS:\n","    folder_path = os.path.join(image_gen_path, emotion)\n","    if os.path.exists(folder_path):\n","        image_files = [f for f in os.listdir(folder_path) if f.endswith(\".png\")]\n","        for image_file in tqdm(image_files, desc=f\"Processing {emotion}\"):\n","            image_path = os.path.join(folder_path, image_file)\n","            pred_label, probs = predict(image_path)\n","\n","            results[emotion].append({\n","                \"image\": image_file,\n","                \"predicted_label\": EMOTIONS[pred_label],\n","                **{EMOTIONS[i]: probs[i] for i in range(len(EMOTIONS))}\n","            })\n","\n","            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìºì‹œ ì •ë¦¬\n","            del pred_label, probs\n","            torch.cuda.empty_cache()\n","\n","# ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì¶œë ¥\n","dfs = {emotion: pd.DataFrame(results[emotion]) for emotion in EMOTIONS}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6MBULUHmUzD","executionInfo":{"status":"ok","timestamp":1739858629419,"user_tz":-540,"elapsed":359412,"user":{"displayName":"ì‹ ì§„ì„­","userId":"17065963932824802147"}},"outputId":"07ff3eba-d516-47df-8618-ce890dc0f973"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing Exciting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 423/423 [00:32<00:00, 13.14it/s]\n","Processing Hopeful: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 397/397 [00:42<00:00,  9.39it/s]\n","Processing Romantic: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 457/457 [00:47<00:00,  9.59it/s]\n","Processing Heartwarming: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [01:03<00:00,  7.60it/s]\n","Processing Calm: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446/446 [00:43<00:00, 10.20it/s]\n","Processing Sad: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 414/414 [00:42<00:00,  9.66it/s]\n","Processing Stress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 423/423 [00:45<00:00,  9.36it/s]\n","Processing Lonely: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 463/463 [00:35<00:00, 12.98it/s]\n"]}]},{"cell_type":"code","source":["for emotion in EMOTIONS:\n","  print(emotion)\n","  print(dfs[emotion]['predicted_label'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CiCfnJEhp_4N","executionInfo":{"status":"ok","timestamp":1739859627512,"user_tz":-540,"elapsed":262,"user":{"displayName":"ì‹ ì§„ì„­","userId":"17065963932824802147"}},"outputId":"93a55bc9-8a59-4c0e-8420-a0ee8ab62497"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Exciting\n","predicted_label\n","Exciting        126\n","Heartwarming    100\n","Hopeful          53\n","Stress           42\n","Romantic         34\n","Lonely           32\n","Calm             24\n","Sad              12\n","Name: count, dtype: int64\n","Hopeful\n","predicted_label\n","Heartwarming    78\n","Hopeful         71\n","Exciting        65\n","Romantic        54\n","Stress          50\n","Lonely          31\n","Calm            29\n","Sad             19\n","Name: count, dtype: int64\n","Romantic\n","predicted_label\n","Romantic        153\n","Heartwarming     70\n","Lonely           64\n","Sad              47\n","Stress           42\n","Hopeful          39\n","Exciting         29\n","Calm             13\n","Name: count, dtype: int64\n","Heartwarming\n","predicted_label\n","Heartwarming    225\n","Exciting         75\n","Stress           47\n","Hopeful          41\n","Romantic         37\n","Lonely           22\n","Sad              17\n","Calm             16\n","Name: count, dtype: int64\n","Calm\n","predicted_label\n","Calm            83\n","Romantic        77\n","Hopeful         66\n","Lonely          53\n","Stress          47\n","Sad             44\n","Exciting        40\n","Heartwarming    36\n","Name: count, dtype: int64\n","Sad\n","predicted_label\n","Lonely          127\n","Sad             108\n","Romantic         42\n","Stress           35\n","Hopeful          30\n","Calm             24\n","Exciting         24\n","Heartwarming     24\n","Name: count, dtype: int64\n","Stress\n","predicted_label\n","Stress          184\n","Lonely           59\n","Sad              49\n","Romantic         43\n","Heartwarming     37\n","Exciting         22\n","Hopeful          21\n","Calm              8\n","Name: count, dtype: int64\n","Lonely\n","predicted_label\n","Lonely          231\n","Sad              74\n","Romantic         43\n","Stress           33\n","Hopeful          29\n","Calm             21\n","Exciting         17\n","Heartwarming     15\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3WPKVnB5uJAS"},"execution_count":null,"outputs":[]}]}